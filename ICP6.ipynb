{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9JtdmWmIhS8d9MENGJVvr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"vqdb4vyL3Brw","executionInfo":{"status":"ok","timestamp":1721709148263,"user_tz":300,"elapsed":15010,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"outputs":[],"source":["import pandas as pd #Basic packages for creating dataframes and loading dataset\n","import numpy as np\n","import matplotlib.pyplot as plt #Package for visualization\n","import re #importing package for Regular expression operations\n","from sklearn.model_selection import train_test_split #Package for splitting the data\n","from sklearn.preprocessing import LabelEncoder #Package for conversion of categorical to Numerical\n","from keras.preprocessing.text import Tokenizer #Tokenization\n","from keras.preprocessing.sequence import pad_sequences #Add zeros or crop based on the length\n","from keras.models import Sequential #Sequential Neural Network\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D #For layers in Neural Network\n","from keras.utils import to_categorical"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset as a Pandas DataFrame\n","dataset = pd.read_csv('/content/sample_data/Sentiment (3) (2).csv')\n","\n","# Select only the necessary columns 'text' and 'sentiment'\n","mask = dataset.columns.isin(['text', 'sentiment'])\n","data = dataset.loc[:, mask]"],"metadata":{"id":"nOqmvRd03Gbt","executionInfo":{"status":"ok","timestamp":1721709247636,"user_tz":300,"elapsed":446,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data['text'] = data['text'].apply(lambda x: x.lower())\n","data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqwtsyuI3J-x","executionInfo":{"status":"ok","timestamp":1721709279451,"user_tz":300,"elapsed":228,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"70c12f29-3f02-42cd-9868-41ebb0a025ce"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-cee1da567eb8>:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data['text'] = data['text'].apply(lambda x: x.lower())\n","<ipython-input-4-cee1da567eb8>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n"]}]},{"cell_type":"code","source":["for idx, row in data.iterrows():\n","    row[0] = row[0].replace('rt', ' ') #Removing Retweets"],"metadata":{"id":"Mfh-bywZ3rcR","executionInfo":{"status":"ok","timestamp":1721709317072,"user_tz":300,"elapsed":1850,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["max_fatures = 2000\n","tokenizer = Tokenizer(num_words=max_fatures, split=' ') #Maximum words is 2000 to tokenize sentence\n","tokenizer.fit_on_texts(data['text'].values)\n","X = tokenizer.texts_to_sequences(data['text'].values) #taking values to feature matrix"],"metadata":{"id":"irFiewtU30PE","executionInfo":{"status":"ok","timestamp":1721709336195,"user_tz":300,"elapsed":1316,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X = pad_sequences(X) #Padding the feature matrix\n","\n","embed_dim = 128 #Dimension of the Embedded layer\n","lstm_out = 196 #Long short-term memory (LSTM) layer neurons\n",""],"metadata":{"id":"o9mwuKdE35CA","executionInfo":{"status":"ok","timestamp":1721709347158,"user_tz":300,"elapsed":158,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def createmodel():\n","    model = Sequential() #Sequential Neural Network\n","    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1])) #input dimension 2000 Neurons, output dimension 128 Neurons\n","    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) #Drop out 20%, 196 output Neurons, recurrent dropout 20%\n","    model.add(Dense(3,activation='softmax')) #3 output neurons[positive, Neutral, Negative], softmax as activation\n","    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy']) #Compiling the model\n","    return model\n","# print(model.summary())"],"metadata":{"id":"Ju1-mYmA37_k","executionInfo":{"status":"ok","timestamp":1721709358185,"user_tz":300,"elapsed":183,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["labelencoder = LabelEncoder() #Applying label Encoding on the label matrix\n","integer_encoded = labelencoder.fit_transform(data['sentiment']) #fitting the model\n","y = to_categorical(integer_encoded)\n","X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42) #67% training data, 33% test data split\n",""],"metadata":{"id":"Mu7pqzVF3-rP","executionInfo":{"status":"ok","timestamp":1721709374412,"user_tz":300,"elapsed":152,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["batch_size = 32 #Batch size 32\n","model = createmodel() #Function call to Sequential Neural Network\n","model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2) #verbose the higher, the more messages\n","score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size) #evaluating the model\n","print(score)\n","print(acc)\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtdAxAln4Cp7","executionInfo":{"status":"ok","timestamp":1721709492316,"user_tz":300,"elapsed":98307,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"c9b63a8b-6ea4-4d7c-c72b-dee34ef0db1d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["291/291 - 56s - loss: 0.8240 - accuracy: 0.6454 - 56s/epoch - 194ms/step\n","144/144 - 6s - loss: 0.7562 - accuracy: 0.6710 - 6s/epoch - 40ms/step\n","0.7562012672424316\n","0.6710354089736938\n"]}]},{"cell_type":"code","source":["print(model.metrics_names) #metrics of the model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbQUHboK4HeG","executionInfo":{"status":"ok","timestamp":1721709492317,"user_tz":300,"elapsed":23,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"23d2481d-afc1-4770-d1a4-5ea0d8ce1c9a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['loss', 'accuracy']\n"]}]},{"cell_type":"code","source":["model.save('sentimentAnalysis.h5') #Saving the model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tY4Qhvo14Obd","executionInfo":{"status":"ok","timestamp":1721709492317,"user_tz":300,"elapsed":17,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"71727521-3751-4a3f-ce79-511dbf31339e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}]},{"cell_type":"code","source":["from keras.models import load_model #Importing the package for importing the saved model\n","model= load_model('sentimentAnalysis.h5') #loading the saved model"],"metadata":{"id":"h6WNKUw-4SXE","executionInfo":{"status":"ok","timestamp":1721709492666,"user_tz":300,"elapsed":361,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print(integer_encoded)\n","print(data['sentiment'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GgatPpdo4VS_","executionInfo":{"status":"ok","timestamp":1721709492666,"user_tz":300,"elapsed":15,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"34b531d5-0d58-4477-aed4-c45e02a53157"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 2 1 ... 2 0 2]\n","0         Neutral\n","1        Positive\n","2         Neutral\n","3        Positive\n","4        Positive\n","           ...   \n","13866    Negative\n","13867    Positive\n","13868    Positive\n","13869    Negative\n","13870    Positive\n","Name: sentiment, Length: 13871, dtype: object\n"]}]},{"cell_type":"code","source":["# Predicting on the text data\n","sentence = ['A lot of good things are happening. We are respected again throughout the world, and that is a great thing.@realDonaldTrump']\n","sentence = tokenizer.texts_to_sequences(sentence) # Tokenizing the sentence\n","sentence = pad_sequences(sentence, maxlen=28, dtype='int32', value=0) # Padding the sentence\n","sentiment_probs = model.predict(sentence, batch_size=1, verbose=2)[0] # Predicting the sentence text\n","sentiment = np.argmax(sentiment_probs)\n","\n","print(sentiment_probs)\n","if sentiment == 0:\n","    print(\"Neutral\")\n","elif sentiment < 0:\n","    print(\"Negative\")\n","elif sentiment > 0:\n","    print(\"Positive\")\n","else:\n","    print(\"Cannot be determined\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSMQ7uHm4ZQO","executionInfo":{"status":"ok","timestamp":1721709492995,"user_tz":300,"elapsed":341,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"b910ef41-cf78-4b21-a6b8-26dc90d8b81c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 - 0s - 302ms/epoch - 302ms/step\n","[0.5344944  0.17772105 0.2877846 ]\n","Neutral\n"]}]},{"cell_type":"code","source":["!pip install scikeras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"id":"1jfiuDfS4coR","executionInfo":{"status":"ok","timestamp":1721709528392,"user_tz":300,"elapsed":18468,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"43c0c660-2155-4ae6-a77a-cc25320f018d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikeras\n","  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n","Collecting keras>=3.2.0 (from scikeras)\n","  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-learn>=1.4.2 (from scikeras)\n","  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n","Collecting namex (from keras>=3.2.0->scikeras)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n","Collecting optree (from keras>=3.2.0->scikeras)\n","  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n","Installing collected packages: namex, optree, scikit-learn, keras, scikeras\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-3.4.1 namex-0.0.8 optree-0.12.1 scikeras-0.13.0 scikit-learn-1.5.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]},"id":"d0480b75f4524edf8b7415e5c6894364"}},"metadata":{}}]},{"cell_type":"code","source":["from scikeras.wrappers import KerasClassifier #importing Keras classifier\n","\n","from sklearn.model_selection import GridSearchCV #importing Grid search CV\n","\n","model = KerasClassifier(model=createmodel,verbose=2) #initiating model to test performance by applying multiple hyper parameters\n","batch_size= [10, 20, 40] #hyper parameter batch_size\n","epochs = [1, 2] #hyper parameter no. of epochs\n","param_grid= {'batch_size':batch_size, 'epochs':epochs} #creating dictionary for batch size, no. of epochs\n","grid  = GridSearchCV(estimator=model, param_grid=param_grid) #Applying dictionary with hyper parameters\n","grid_result= grid.fit(X_train,Y_train) #Fitting the model\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) #best score, best hyper parameters\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"id":"uANoMJeu4kp2","executionInfo":{"status":"error","timestamp":1721709541386,"user_tz":300,"elapsed":366,"user":{"displayName":"Anvesh Gangishetti","userId":"07484702439411594011"}},"outputId":"2bcf6481-bfa2-44f5-c03a-f445ff611401"},"execution_count":17,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-03eef86d9b65>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m \u001b[0;31m#importing Keras classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m \u001b[0;31m#importing Grid search CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreatemodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#initiating model to test performance by applying multiple hyper parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierLabelEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressorTargetEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scikeras/utils/transformers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}